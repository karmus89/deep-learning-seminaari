{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Created by Petteri Nevavuori.*\n",
    "\n",
    "---\n",
    "\n",
    "# Deep Learning seminaari\n",
    "\n",
    "Kirjana Goodfellow et al.: Deep Learning (2016)\n",
    "\n",
    "Otsikot seuraavat pääotsikoiden tasolla kirjaa, mutta alaotsikot eivät aina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#14.-Autoencoders\" data-toc-modified-id=\"14.-Autoencoders-1\">14. Autoencoders</a></span><ul class=\"toc-item\"><li><span><a href=\"#14.1-Undercomplete-Autoencoders\" data-toc-modified-id=\"14.1-Undercomplete-Autoencoders-1.1\">14.1 Undercomplete Autoencoders</a></span></li><li><span><a href=\"#14.2-Regularized-Autoencoders\" data-toc-modified-id=\"14.2-Regularized-Autoencoders-1.2\">14.2 Regularized Autoencoders</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sparse-Autoencoders\" data-toc-modified-id=\"Sparse-Autoencoders-1.2.1\">Sparse Autoencoders</a></span></li><li><span><a href=\"#Denoising-Autoencoders\" data-toc-modified-id=\"Denoising-Autoencoders-1.2.2\">Denoising Autoencoders</a></span></li><li><span><a href=\"#Regularizing-by-Penalizing-Derivatives\" data-toc-modified-id=\"Regularizing-by-Penalizing-Derivatives-1.2.3\">Regularizing by Penalizing Derivatives</a></span></li></ul></li><li><span><a href=\"#14.3-Representational-Power,-Layer-Size-and-Depth\" data-toc-modified-id=\"14.3-Representational-Power,-Layer-Size-and-Depth-1.3\">14.3 Representational Power, Layer Size and Depth</a></span></li><li><span><a href=\"#14.4-Stochastic-Encoders-and-Decoders\" data-toc-modified-id=\"14.4-Stochastic-Encoders-and-Decoders-1.4\">14.4 Stochastic Encoders and Decoders</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 14. Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Autoenkooderit ovat neuroverkkoja, jotka pyrkivät oppimaan kopioimaan syötteitä niin hyvin, kuin mahdollista. Käytännössä siinä on piilokerros $h$, joka toimii syötteen ja ulostulon välisenä avaimena tai koodina. Verkko koostuukin kahdesta osasta, enkoodaavasta ja dekoodaavasta verkosta. Enkoodaava funktio $f(x)=h$ muuntaa syötteen piilokerrokseen ja dekoodaava funktio $g(h)=r$ taas pyrkii rakentamaan syötettä vastaavan signaalin piilokerroksesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Verkkojen käyttökelpoisuus tulee siitä, että niiden ei sallita päästää syötettä läpi vain sellaisenaan, milloin $g(f(x))=x$. Ne pakotetaan priorisoimaan syötteen eri osia estämällä niitä oppimasta kopiointia täydellisesti. Näin ne myös oppivat merkityksellisiä piirteitä datasta. Verkot ovat myötäkytkettyjen verkkojen erikoistapaus, minkä vuoksi ne voidaan kouluttaa aivan kuten mikä tahansa muukin perinteinen neuroverkko. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Modernit autoenkooderit toimivat determinististen funktioiden lisäksi myös stokastisten funktioiden kanssa. Verkkojen idea on sinänsä melko vanha, mutta generatiiviseen mallinnukseen käytettynä verkkojen käyttö- ja tutkimusmäärät ovat nousseet nykyisiin mittakaavoihin. Alussa verkkoja käytettiin piirravaruuden kaventamiseen tai piirteidein oppimiseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 14.1 Undercomplete Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Autoenkooderiverkkojen piilokerros $h$ on niiden kaikkein kiinnostavin osa. Aiemmin mainittu täydellisen oppimisen estäminen tapahtuu rajoittamalla piilokerroksen kokoa siten, että sen yksikköjen määrä on syötteen piirremäärää pienempi. Tällöin puhutaan vajaista (*undercomplete*) autoenkoodereista. Tällaisen verkon kouluttaminen pakottaa verkon oppimaan merkityksellisimmät datan piirteet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Autoenkooderit mittaavat virhettä niiden syötteen $x$ ja verkon ulostulon $g(f(x))$ välillä. Virhefunktio, joka rankaisee verkkoa syötteistä eroavista ulostuloista, on tällöin siis $ L(x, g(f(x))) $. Lineaarisena autoenkooderi oppii koulutuksen sivutuotteena PCA:n tapaisen signaalien aliavaruuden. Se on siis vähempipiirteinen avaruus, jossa monimutkainen signaali on hajoitettu muutamaksi merkitykselliseksi osasignaaliksi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mikäli dekooderi on epälineaarinen, kykenevät verkot oppimaan vieläkin tehokkammaan yleistyksen PCA:sta. Liikaa kapasiteettia verkolla ei kuitenkaan saa olla, sillä muutoin se oppii kopioinnin liian hyvin, eikä merkityksellisimpien piirteiden poiminta ole enää onnistunutta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 14.2 Regularized Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kun piiloyksiköiden määrä vastaa tai ylittää syötteen piirteiden määrän, puhutaan ylitäydellisistä (*overcomplete*) autoenkoodereista. Tällöin verkon on kaikista kevyintä oppia vain suoraan kopioimaan syöte sellaisenaan, eikä merkityksellisien piirteiden oppimista tapahdu ollenkaan. Siksi pelkkä virhefunktion tarkkailu ei ole riittävää näiden verkkojen kanssa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regularisoimalla autoenkoodereita voidaan mallin kapasiteettia säätää mallinnettavaan ongelmaan sopivaksi. Regularisoinnilla mallin koulutukselle saadaan asetettua myös muita hyödyllisiä tavoitteita, kuin vain syötteen onnistunut kopiointi. Näitä ovat esimerkiksi robustisuus kohinalle ja opittujen piirteiden harvuus (*sparsity of representations*). Regularisoituna ylitäydellinenkin autoenkooderi voi oppia datasta jotain merkityksellistä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Melkein mikä tahansa datapohjainen piilomuuttujia hyödyntävä generatiivinen malli voidaan nähdä jonkinasteiesena autoenkooderina. Kirjan myöhemmissä luvuissa esitellään kaksi tällaista mallia, muuttuva (*variational*) autoenkooderi ja genertiivinen stokastinen verkko (*generative stochastic network*). Nämä oppivat luontaisesti hyödyllisen korkeakapasiteettisen ja ylitäydellisen syötteen enkoodauksen ilman regularisoinnin tarvetta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Harva autoenkooderi on yksinkertaisuudessaan autoenkooderi, jonka piilokerrokselle $h$ asetataan harvuutta suosiva regularisointisakko. Tällaisen verkon kohdefunktio muistuttaa paljon regularisoidun myötäkytketyn verkon kohdefunktiota. Kaavana se voidaan ilmaista muodossa\n",
    "\n",
    "$$L(x, g(f(x)))+\\Omega(h),$$\n",
    "\n",
    "jossa $\\Omega(h)$ on piilokerroksen regularisointifunktio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Harvuutta suosiva regularisointifunktio eroaa kuitenkin esimerkiksi $L^2$-normisakosta. Perinteisesti pyritään maksimoimaan $p(\\theta \\mid x)=\\log{p(x \\mid \\theta)} + \\log{p(\\theta)}$, missä parametreihin kohdistuu erillinen oletus jakaumasta. Autoenkooderin regularisointi riippuu kuitenkin datasta eikä parametreista. Harvan autoenkooderin regularisointi voidaankin ennemmin nähdä piilomuuttujia sisältävän generatiivisen mallin kouluttamisena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Autoenkooderin harvuus on samankaltaista edellisen luvun harvan koodauksen kanssa. Harvan koodauksen todennäköisimmän $h$:n optimoinnin sijasta harvan autoenkooderin kanssa $h$ opitaan enkooderin ulostulona. Harvan koodauksen tavoin autoenkooderin piiloyksiköiden jakaumalla voidaan saada jo harvoja piiloyksiköitä ja piirteitä. Kuten harvassa koodauksessa, todellisesti nolla-arvoisia piirteitä ei kuitenkaan ole monia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Harvuus on autoenkooderien tapauksessa kuitenkin seurausta kohdefunktiossa käytetystä piiloyksiköiden regularisointiparametrista. Koska kohdefunktiota pyritään minimoimaan, mallin kohdefunktio paranee, kun piilokerrokset yksiköt ovat harvempia eli harvemmin aktiivisia tai pieniarvoisia. Näin koulutettu malli on myös vain datantuottoprosessia approksimoiva, eikä suoraan $x \\to x$ kopioiva. Näin koulutetun verkon piiloyksiköt toimivat syötettä selittävinä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularisoinnin sijasta autoenkooderien kouluttumiseen voidaan vaikuttaa myös virhefunktion suunnittelulla. Perinteisesti virhefunktio lasketaan syötteen $x$ ja tuotteen $g(f(x))$ välillä. Vaihtoehtoisesti malli voidaan kouluttaa käyttämällä syötteenä kohinaista syötettä $\\tilde{x}$, jolloin virhfunktioksi muodostuu\n",
    "\n",
    "$$L(x, g(f(\\tilde{x}))).$$\n",
    "\n",
    "Näin koulutettua verkkoa kutsutaan kohinaa poistavaksi autoenkooderiksi (*denoising autoencoder*). Pelkän syötteen kopioinnin oppimisen sijasta verkon on myös opittava poistamaan lisätty kohina. Näin malli oppii paremmin syötedatan jakauman rakenteen ja samalla merkittävät piirteet ikäänkuin sivutuotteena. Nämä verkot esitellään tarkemmin aliluvussa 14.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Regularizing by Penalizing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kun regularisointitermiin lisätään piilokerroksen $h$ lisäksi myös syöte $x$, saadaan malli oppimaan syötteen muutosnopeuteen mukautuvaksi. Tällaista autoenkooderia kutsutaan supistuvaksi (*contractive*). Tämäkin malli esitellään tarkemmin myöhemmin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 14.3 Representational Power, Layer Size and Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Usein autoenkooderit koulutetaan matalina, eli yhdellä enkooderi- ja yhdellä dekooderikerroksella. Syvyyden lisäämisellä on kuitenkin etuja, samaan tapaan kuin perinteisillä myötäkytketyillä verkoilla. Koska autoenkooderin osaverkot ovat omia verkkojaan, kumpikin osaverkoista hyötyy syvyyden lisäyksestä erikseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Syvyyden lisäys voi ensinnäkin helpottaa verkon koulutuksen laskennallisia vaatimuksia helpottamalla datantuottoprosessin kasvatetun kapasiteetin seurauksena. Samoin kasvatettu syvyys vähentää koulutusdatasetin minimikokovaatimusta. Syvät autoenkooderit pystyvät myös puristamaan eli enkoodaamaan datantuottprosessin tehokkaammin. Näiden verkkojen koulutus tapahtuu usein esikouluttamalla matalampia verkkoja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 14.4 Stochastic Encoders and Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "318px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
