{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Created by Petteri Nevavuori.*\n",
    "\n",
    "---\n",
    "\n",
    "# Deep Learning seminaari\n",
    "\n",
    "Kirjana Goodfellow et al.: Deep Learning (2016)\n",
    "\n",
    "Otsikot seuraavat pääotsikoiden tasolla kirjaa, mutta alaotsikot eivät aina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#15.-Representation-Learning\" data-toc-modified-id=\"15.-Representation-Learning-1\">15. Representation Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#15.1-Greedy-Layer-Wise-Unsupervised-Pretraining\" data-toc-modified-id=\"15.1-Greedy-Layer-Wise-Unsupervised-Pretraining-1.1\">15.1 Greedy Layer-Wise Unsupervised Pretraining</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-and-Why-Does-Unsupervised-Pretraining-Work?\" data-toc-modified-id=\"When-and-Why-Does-Unsupervised-Pretraining-Work?-1.1.1\">When and Why Does Unsupervised Pretraining Work?</a></span></li></ul></li><li><span><a href=\"#15.2-Transfer-Learning-and-Domain-Adaptation\" data-toc-modified-id=\"15.2-Transfer-Learning-and-Domain-Adaptation-1.2\">15.2 Transfer Learning and Domain Adaptation</a></span></li><li><span><a href=\"#15.3-Semi-Supervised-Disentangling-of-Causal-Factors\" data-toc-modified-id=\"15.3-Semi-Supervised-Disentangling-of-Causal-Factors-1.3\">15.3 Semi-Supervised Disentangling of Causal Factors</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 15. Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tässä luvussa keskitytään kuvausten (*representation*) oppimisen merkityksellisyyteen ja käyttökelpoisuuteen syvien menetelmien kanssa. Luvussa käydään läpi jaettuja kuvauksia, kuten ohjaamattomien menetelmien hyödyntämistä ohjatussa oppimisessa. Jaettuja kuvauksia käsitellään laajemminkin, minkä jälkeen lopussa käydään läpi kuvauksiin eli piirteiden selvittämiseen pohjaavien mallien menestyksen syitä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Informaation prosessointitehtävien haasteellisuus rippuu datan käsittely- ja esitystavasta. Tämä päätee elämässä laajemminkin. Laskutoimitukset hankaloituvat, mikäli pakotetaan roomalaisten numeroiden käyttö ja lukujen listaan lisäämisen tehokkuus vaihtelee listan esitystavan mukaan. Koneoppimisessa datan esitystapa on hyvä, kun se tekee käsiteltävän haasteen oppimisesta helpompaa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perinteisissä myötäkytketyissä verkoissa piilokerrosten tehtävänä on tarjota viimeiselle eli ulostulokerrokselle helposti käytettävä esitys eli kuvaus syötteestä. Ohjatun oppimisen tapauksessa mallin oppimille piilokuvauksille ei ole rajoitteita. On kuitenkin myös algoritmeja, jotka pyrkivät muokkamaan dataa tiettyihin esitysmuotoihin. Jotkut mallit, kuten ohjaamattomat, oppivat datan kuvauksia koulutuksen sivutuotteena. Joka tapauksessa opittu kuvaus on lähes aina uudelleenkäytettävissä jossain toisessa oppimistehtävässä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kuvausten oppiminen tarjoaa keinoja sekä ohjaamattoman että semi-ohjatun oppimisen tekemiseen, sillä oikein merkittyä dataa ei aina ole riittävästi koko datamassaan nähden yleistyvän ohjatun oppimisen suorittamiseksi. Ihmisten ja eläinten hyvin tehokkaaseen oppimiskykyyn nojaten on yhtenä yleisenä hypoteesinä, että aivoissakin toimisi jossain määrin ohjaamattoman ja semi-ohjatun oppimisen prosesseja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 15.1 Greedy Layer-Wise Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ohjaamaton oppiminen oli merkittävin syväoppimisen tutkimuksen henkiinherättäjä etenkin ahneen kerroskohtaisen ohjaamattoman esikoulutuksen (*greedy layer-wise unsupervised pretraining*) vuoksi. Se on ennenkaikkea ohjaamattoman oppimisen käyttöä hyödyllisten kuvausten löytämiseksi, joiden avulla syvempien ohjatusti oppivien verkkojen koulutus on sitten tehokkaampaa ja ylipäätään mahdollista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ohjaamaton esikoulutus tehdään kuvauksia tehokkaasti oppivilla matalilla menetelmillä, kuten rajoitetuilla Boltzmannin koneilla, autoenkoodereilla tai jollakin harvan koodauksen menetelmällä. Näillä menetelmillä esikoulutetaan syvän tavoiteverkon jokainen kerros yksitellen. Näin voidaan välttää resurssien osalta paljon intensiivisempää koko verkon kerralla koulutusta. \n",
    "\n",
    "Vasta 2000-luvun puolivälin jälkeen huomattiin, että näin voidaan toimia ja toimittaessa saadaan hyvin alustettuja ja täten tehokkaasti kouluttuvia verkkoja, joskin tämä ei ole enää lähtökohtainen vaatimus syvien verkkojen koulutukseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Menetelmää kutsutaan kerroskohtaisesti ahneeksi, koska se kouluttaa jokaisen kerroksen itsenäisesti huomioimatta muita kerroksia. Koulutus eteneekin siten, että koulutuksen aikana kaikki muut paitsi koulutettava kerros on jäädytetty eli pysyy muuttumattomana. Ohjaamattomaksi esikoulutusmenetelmäksi sitä kutsutaan siksi, että kukin kerros vain esikoulututeaan koulutettavaksi osaksi suurempaa verkkoa käyttäen ohjaamattoman oppimisen menetelmiä.\n",
    "\n",
    "Riippumatta valitusta esikoulutusalgoritmista itse esikoulutusprosessi on aina samankaltainen. Tämä on seurausta etenkin ohjaamattomien menetelmien samankaltaisuuksista. Ohjattujen menetelmien alustuksen lisäksi ohjaamatonta esikoulutusta voidaan käyttää myös ohjaamattomien verkkojen alustukseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### When and Why Does Unsupervised Pretraining Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ohjaamattoman esikoulutuksen on huomattu parantavan testivirhettä etenkin luokittelutehtävissä. Muissa koneoppimistehtävissä hyöty voi kuitenkin jäädä olemattomaksi tai jopa siirtyä haitan puolelle. Regressiotehtävissä sen on huomattu olevan esimerkiksi ennemmin haitallista. Siksi sen, eli siis ohjaamattoman kerroskohtaisen ahneen esikoulutuksen, käyttö edellyttää menetelmän ymmärrystä ja harkintaa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ohjaamaton esikoulutus yhdistää kaksi erillistä ajatusta. Ensimmäinen niistä on, että syvän mallin parametrien alustuksella on merkitystä mallin koulutuksen ja lopullisen suorituskyvyn kannalta regularisoivuuden kautta. Toinen on, että syötedatan jakaumasta oppimalla voidaan edesauttaa mallin syötteistä tuotteisiin tapahtuvaa kuvauksen tarkentumista. Molemmat näistä omaavat koneoppimisen alueella riippuvuuksia, joita ei täysin ymmärretä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Parametrien alustuksen regularisaatiovaikutus on näistä kahdesta huonommin ymmärretty. Esikoulutuksen alkuvaiheessa tavoite oli mallin alustaminen lähelle globaalisti kohdefunktion arvon minimoivaa parametriavaruuden kohtaa. Nykyään lokaalit minimit eivät kuitenkaan ole enää merkittäviä esteitä, sillä koulutus ei saavuta lähes koskaan mitään kriittistä pistettä, vaan vain lähestyy sitä.\n",
    "\n",
    "Koska alustuksen vaikutusta koulutukseen ei osata riittävän tarkasti rajata, ei alustuksella välttämättä saavuteta muuta kuin mahdollisesti muutoin saavuttamattomia alueita. Esimerkiksi tietoa siitä, mitkä alustetut piirteet säilyvät ja mitkä eivät, ei voida saavuttaa. Siksi onkin tyypillistä, että esikoulutuksen sijasta ohjaamattomia ja ohjattuja menetelmiä käytetään koulutuksessa rinnan tai jopa siten, että menetelmät erotetaan mallin omiksi osikseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Jälkimmäinen ajatuksista, ohjaamattomasti opittujen datan piirteiden myöhempi hyödynnettävyys, on paremmin ymmärretty. Perusajatuksena on, että ohjaamattomalle menetelmälle hyödylliset piirteet voivat olla hyödyllisiä myös ohjatulle menetelmälle. Esimerkiksi hahmontunnistuksessa tästä voi olla hyötyä, kun ensin ollaan opittu olennaisesti toisistaan eroavia piirteitä ohjaamattomasti ja tämän jälkeen näitä käytetään vaikkapa luokitteluun.\n",
    "\n",
    "Teoreettisella tasolla tätä ei kuitenkaan vielä ymmärretä, vaan vain empiirisellä ja intuitiivisella. Ajatuksen hyödynnettävyys riippuu myös jonkin verran käytetyistä malleista - esimerkiksi lineaariluokitin tarvitsee lineaarisesti erottuvia piirteitä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ohjaamaton esikoulutus on parhaimmillaan silloin, kun datan lähtökohtainen kuvauksellisuus on heikkoa. Esimerkiksi sanojen olemassaolon binäärimäinen ilmaisu (*one-hot encoding*) ei auta juurikaan niiden luokittelussa. Tällöin ohjaamattomasti opitut sanaupotteet (*word embeddings*) ovat paljon parempi ja informaatiota tehokkaammin välittävä vaihtoehto. Ylipäätään kielen ja sanojen kanssa esikoulutus on hyödyllistä. Kuvien kanssa hyöty ei ole niin suurta kuvauksiltaan lähtökohtaisesti paljon rikkaammaan datan vuoksi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kun datassa on vain vähän oikein merkittyjä (*labeled*) näytteitä suhteessa merkitsemättömiin näytteisiin, auttaa ohjaamaton esikoulutus regularisoimaan koulutusta. Tällöin merkitsemättömät näytteet saadaan ryhmiteltyä, jolloin saavutetaan semi-ohjattu koulutustilanne. Tämä toimintatapa sai suurimman vahvistuksen vuonna 2011, kun sitä käyttämällä voitettiin kansainvälisiä siirtokoulutukseen (*transfer learning*) liittyviä kilpaluja.\n",
    "\n",
    "Mikäli opittava ongelma eli funktio kovin monimutkainen, on ohjaamaton esikoulutus vaikkapa painojen heikentämistä parempi regularisointimenetelmä. Vääristymän sijasta esikoulutus auttaa mallia suuntaan, jossa monimutkaiselle funktiolle voidaan löytää merkittäviä piirre- eli osafunktioita. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ohjaamatonta esikoulutusta on käytetty menestyksekkäimmin luokittelussa ja regularisoijana, eli testivirheen laskijana. Pelkän regularisoinnin lisäksi menetelmä auttaa myös optimoinnissa, jolloin koulutus on tehokkaampaa. Näin on havaittu vaikkapa syvien autoenkooderien kanssa. Tällöin sekä koulutus- että testivirheet saadaan tuotua matalammalle tasolle, jolloin verkon kapasiteettia hyödynnetään paremmin. \n",
    "\n",
    "Syyksi tälle on esitetty, että esikoulutus vie mallin parametrit muutoin koulutuksen ulottumattomissa olevalle alueelle. Esikoulutetut neuroverkot näyttävät käyttäytyvän samaan tapaan monista muuttuvista tekijöistä huolimatta - ne tuottavat samankaltaisia tuloksia. Näyttää siltä, että esikoulutus pienentää verkkojen varianssia. Tämän voidaan tulkita tarkoittavan sitä, että ohjaamaton esikoulutus alustaa mallin parametrit alueelle, jossa ne pysyvät vahvemmin.\n",
    "\n",
    "Hyödyllisimmillään ohjaamaton esikoulutus on syvien arkkitehtuurien kanssa, sillä niiden kanssa on havaittu suurimmat koulutus- ja testivirheiden parannukset. Nämä tulokset ovat kuitenkin saatu ennen nykyisiä syvien verkkojen koulutuksen standardimenetelmiä (*ReLU, dropout, batch normalization*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Muihin regularisointimenetelmiin verrattuna ohjaamattomalta esikoulutukselta puuttuu koulutuksen aikaisen vahvuuden säätöominaisuus hyperparametrein. Sen sijaan itse esikoulutusmenetelmillä on useita omia hyperparametrejaan, joita on hankala virittää etukäteen. Koko mallin koulutus on myös kaksivaiheinen. Mikäli ohjattua ja ohjaamatonta koulutusta käytetään rinnan, voidaan ohjaamattoman koulutuksen vahvuutta säätää. Tällöin regularisoinnin määrä on ennustettavissa ja malli koulutettavissa kerralla. Näin ei siis kuitenkaan ole ohjaamattoman esikoulutuksen kanssa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kaksivaiheinen koulutus nostaa myös lisäongelmia hyperparametrien osalta. Koska varsinainen koulutus seuraa esikoulutusta ja riippuu myös siitä, on varsinaisen koulutuksen hyperparametrien säätäminen tehtävä aina suhteessa esikoulutukseen. Viritys on tällöin raskasta ja hidasta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ohjaamaton esikoulutus on kuitenkin pääsääntöisesti nykyaikaina jäänyt unholaan, poislukien luonnollisen kielen koneoppimistehtävät. Ohjattuun oppimiseen perustuvat ja joko poispudotuksella tai osajoukkojen normalisoinnilla regularisoidut syväoppivat menetelmät pärjäävät ohjaamattomia esikoulutusmenetelmiä paremmin vielä muutaman tuhannen oikein merkityn näytteen luokittelutehtävissä. Pienemmillä dataseteillä Bayesilaiset menetelmät toimivat vuorostaan paremmin.\n",
    "\n",
    "Menetelmän merkityksellisyys onkin historiallinen, ja jo luvussa 8 esitetty ohjattu esikoulutus onkin nykyisinkin käytetty esikoulutusmenetelmä siirto-oppimiseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 15.2 Transfer Learning and Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Siirto-oppiminen (*tranfer learning*) ja tietoalueeseen sopeutuminen (*domain adaptation*) viittaavat toisaalla opitun hyödyntämiseen muussa yhteydessä. Aihe liittyy läheisesti kuvausten jakamiseen mallien välillä. Käyttötarkoituksesta riippuen aihetta sivuavia ongelman nimityksiä on useita. Tavoitteena joka tapauksessa on, että aiemmasta kontekstista opitut kuvaukset ovat käytettävissä joko sellaisinaan tai täydennettyinä jossain toisessa riittävän samankaltaisessa kontekstissa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Siirto-oppimisessa lähtökohtana, että opitut piirteet ja datan varianssia selittävät tekijät ovat hyödyllisiä myös toisessa ajatellussa kontekstissa. Tällöin vaikkapa syötteet pysyvät samoina, mutta ennustettavat kohteet eivät. Hahmontunnistuksessa tietyt reunat, valoisuuden vaihtelut ja geometriat ovat pääpiirteisesti samoja, vaikka yksityiskohdat olisivatkin eriäviä. Näin tarkaksi koulutettu koiria tunnistava malli voi toimia hyvänä lähtökohtana vaikkapa kissoja tunnistavalle mallille. Tällöin mallista on hyödyllistä käyttää syöte- ja piilokerroksia uudelleen.\n",
    "\n",
    "Toisaalta ennustettavat kohteet voivat myös pysyä samoina syötteiden vaihtuessa, joko piilo- tai ulostulokerroksissa. Tällöin uudelleenkäytön kohteita ovat piilo- ja ulostulokerrokset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tietoalueeseen sopeutuminen on sukua siirto-oppimiselle. Siinä tosin malli pysyy samana, mutta syötteissä on eroa. Tällöin käyttökohteena voi olla vaikkapa tunneanalyysi tuotearvosteluissa, jossa samaa asiaa tahdotaan tunnistaa eri konteksteissa. Tunnistunmisen kohde ei siis muutu, ja oletuksena myös on, että kontekstin vaihtuessa tietyt tunnetiloja ilmaisevat merkitsevät piirteet pysyvät samoina.\n",
    "\n",
    "Edelleen saman alueen ongelma on konseptin liukuma (*concept drift*), joka on pohjimmiltaan tietoalueeseen sopeutumista mutta asteittaisin datan jakauman muutoksin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Siirto-oppimisen alueen koneoppimiskilpailuissa on havaittu, että mitä syvemmillä verkoilla eli syvempiä piirteitä ensimmäisestä kontekstista optiaan, sitä paremmin ja nopeammin malli kykenee oppimaan toisessa kontekstissa. Selityksenä voi olla, että tällöin pienempi määrä näytteitä riittää lähes asymptoottisen yleistymisen oppimiseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Äärimmäisiä siirto-oppimisen erikoistapauksia ovat kerrasta (*one-shot*) ja näkemättä (*zero-shot, zero-data*) oppiminen. Näille annetaan nimensä mukaisesti joko yksi tai ei yhtään merkittyä näytettä, josta oppiminen tapahtuu. Kerrasta näkeminen nojaa ensimmäisestä kontekstista opittuihin, jonka avulla uuden kontekstin näytteet voidaan kerralla sijoittaa ja ryhmitellä. Tällöin tärkeintä on selkeästi erottuvat ja merkitykselliset opitut piirteet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Näkemättä oppiminen vaatii datasetin lisäksi aina jotain lisätietoa. Malli voi esimerkiksi oppia tunnistamaan kissan kuvasta, jos se saa tekstistä tietoa kissan jaloista, kasvoista ja vaikkapa hännästä. Tällöin mallin on kyettävä kuitenkin yhdistämään nämä yksittäiset piirteet kuvista havaittaviin hahmon osiin todetakseen kokonaisuuden olevan melko varmasti kissa. \n",
    "\n",
    "Mikäli kyseessä olisi lauseiden tunnistus näkemättä, olisi mallilla oltava käytössään riittävästi sanojen ja lauseiden merkityksiä määrittäviä sanaupotteita. Konekäännöksessä tämä tarkoittaisi lähde- ja kohdekielten sanojen kielten sisäisten ja välisten yhteyksien oppimista yhdessä joidenkin samaa tarkoittavien lauseiden kanssa. Tällöin ennalta tuntemattoman lauseen kääntäminen opettamatta voi olla mahdollista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Näkemättä oppiminen toimii samalla periaatteella, kuin monimerkityksellisyyksien oppiminen (*multimodal learning*). Siinä pyritään tavoittamaan kahden eri merkityksen kuvauksen välinen suhde. Kun tämä suhde on opittu, ovat kuvaukset ankkuroitu toisiinsa ja niitä voidaan käyttää kiintopisteinä uusien jompaan kumpaan liittyvän merkityksen kuvauksen oppimisessa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 15.3 Semi-Supervised Disentangling of Causal Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tärkeä kuvausten oppimiseen liittyvä kysymys liittyy kuvausten paremmuuden mittariin. Eräs ajatuksista on, että hyvät kuvaukset kykenevät erottelemaan itsenäisiä kausaalisuuksia eli datantuottoprosessissa perustavasti vaikuttavia syitä. Itsenäisyydellä tarkoitetaan, että kausaalisuudet ovat toisistaan erillisiä. Vaikka tästä ei suoraan sitä seuraakkaan, niin hyvä kuvaus voi myös helpottaa syötteen laskentaa tuotteeksi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kuvausten oppimisen tavoitteena on usein ollut helposti mallinnettavien eli harvojen ja riippumattomien piirteiden löytäminen, sillä täysin kausaalisuudet erottavan mallin tuottaminen ei ole helppoa. Semi-ohjattu oppiminen ohjaamattomien kuvausten oppimisen kautta tarjoaa kuitenkin välineitä erotella kausaalisuudet toisistaan, kunhan vain ensin datantuottoprosessin merkittävät piirteet on opittu.\n",
    "\n",
    "Toisin sanoen, mikäli datasta $x$ opitaan piirteet $h$, voidaan piirteillä $h$ ennustaa merkityksellisimmät piirteet kohteista $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Semi-ohjattu oppiminen voi silti epäonnistua, mikäli datasta ei voida löytää mitään merkityksellistä vaikkapa datan tasaisen jakautuneisuuden vuoksi. Mikäli datantuottoprosessi on kuitenkin useamman jakauman yhdistelmä siten, että kohdearvot ovat näiden yhdistelmäjakaumien tuotoksia, voidaan datantuottoprosessia mallintamalla erottaa lopullisen jakauman komponentit selkeästi toisistaan. Tällöin myös yksi merkitty näyte riittää kunkin osa-jakauman mallintamiseen.\n",
    "\n",
    "Mikäli kohdearvot ovat vielä yhteydessä datantuottoprosessin kausaalisiin tekijöihin, on datantuottoprosessille ja syötteiden kohteiksi mallintamisen välillä vahva sidos. Tällöin datantuottoprosessin variaatioita eniten selittävät tekijät on löydettävissä sekä ohjaamattomilla että semi-ohjatuilla menetelmillä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "318px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
